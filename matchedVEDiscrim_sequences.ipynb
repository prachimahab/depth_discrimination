{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Verbal Judgement Sequences for VE Discrim Coding\n",
    "\n",
    "For each existing discrimination sequence, make a corresponding VE sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/depth_discrimination/TAC_depth_discrimination_MTurk/discrim_jsons/v2_shuffled_g0_dr.json'\n",
    "\n",
    "with open(p0) as f:\n",
    "    v2_shuffled_g0_dr = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VE_v2_shuffled_g0_dr = []\n",
    "\n",
    "for trial in v2_shuffled_g0_dr:\n",
    "    if trial['sequence'] != 'catch_trial':\n",
    "        img0 = trial['image_path_target_0']\n",
    "        d0 = trial['depth_0']\n",
    "        img1 = trial['image_path_target_1']\n",
    "        d1 = trial['depth_1']\n",
    "\n",
    "        duration = trial['duration']\n",
    "        VE_v2_shuffled_g0_dr.append([img0, duration, d0])\n",
    "        VE_v2_shuffled_g0_dr.append([img1, duration, d1])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VE_v2_shuffled_g0_dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['depth_discrimination_stimuli/001618_2014-06-20_11-37-31_260595134347_rgbf000027-resize_3/001618_2014-06-20_11-37-31_260595134347_rgbf000027-resize_3-target.png',\n",
       " 1000,\n",
       " 1.8725]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VE_v2_shuffled_g0_dr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'g0',\n",
       "  'duration': 1000,\n",
       "  'depth_0': 1.8725,\n",
       "  'depth_1': 1.3545,\n",
       "  'image_path_target_0': 'depth_discrimination_stimuli/001618_2014-06-20_11-37-31_260595134347_rgbf000027-resize_3/001618_2014-06-20_11-37-31_260595134347_rgbf000027-resize_3-target.png',\n",
       "  'image_path_target_1': 'depth_discrimination_stimuli/000109_2014-05-14_23-41-52_260595134347_rgbf000035-resize_9/000109_2014-05-14_23-41-52_260595134347_rgbf000035-resize_9-target.png',\n",
       "  'mask_path': 'masks/mask_10.jpg',\n",
       "  'fixation_path': 'fixation.jpg'},\n",
       " {'sequence': 'g0',\n",
       "  'duration': 1000,\n",
       "  'depth_0': 4.105,\n",
       "  'depth_1': 4.388999999999999,\n",
       "  'image_path_target_0': 'depth_discrimination_stimuli/002103_2014-06-25_20-00-40_260595134347_rgbf000049-resize_0/002103_2014-06-25_20-00-40_260595134347_rgbf000049-resize_0-target.png',\n",
       "  'image_path_target_1': 'depth_discrimination_stimuli/002444_2014-06-28_20-32-08_260595134347_rgbf000027-resize_0/002444_2014-06-28_20-32-08_260595134347_rgbf000027-resize_0-target.png',\n",
       "  'mask_path': 'masks/mask_5.jpg',\n",
       "  'fixation_path': 'fixation.jpg'},\n",
       " {'sequence': 'g0',\n",
       "  'duration': 250,\n",
       "  'depth_0': 2.8,\n",
       "  'depth_1': 2.266,\n",
       "  'image_path_target_0': 'depth_discrimination_stimuli/000800_2014-06-08_22-42-09_260595134347_rgbf000095-resize_3/000800_2014-06-08_22-42-09_260595134347_rgbf000095-resize_3-target.png',\n",
       "  'image_path_target_1': 'depth_discrimination_stimuli/000869_2014-06-09_20-50-18_260595134347_rgbf000060-resize_3/000869_2014-06-09_20-50-18_260595134347_rgbf000060-resize_3-target.png',\n",
       "  'mask_path': 'masks/mask_20.jpg',\n",
       "  'fixation_path': 'fixation.jpg'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2_shuffled_g0_dr[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence: \"a\"\n",
    "image: \"/Users/prachi/Documents/depth_duration/mar3_depthDuration_stimuli/targetImages_kinect2data_subset/002750_2014-06-22_19-08-03_094959634447_rgbf000094-resize_4\"\n",
    "duration: 250\n",
    "num: 0\n",
    "depth: 2.354\n",
    "image_path: \"depth_duration_stimuli/002750_2014-06-22_19-08-03_094959634447_rgbf000094-resize_4/002750_2014-06-22_19-08-03_094959634447_rgbf000094-resize_4-original.jpg\"\n",
    "image_path_target: \"depth_duration_stimuli/002750_2014-06-22_19-08-03_094959634447_rgbf000094-resize_4/002750_2014-06-22_19-08-03_094959634447_rgbf000094-resize_4-target.png\"\n",
    "mask_path: \"masks/mask_0.jpg\"\n",
    "fixation_path: \"fixation.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequences Pipeline\n",
    "\n",
    "- start with discrimination sequences \n",
    "- break into 4 blocks and randomly shuffle so that succesive trials are not within the same depth bin \n",
    "- Should images be in the same order across sequences\n",
    "    - might be a good idea to have different image order across sequences\n",
    "    \n",
    "    \n",
    "For discrimination sequences \n",
    "\n",
    "- random order for trials would be better\n",
    "\n",
    "\n",
    "Verbal judgement sequences \n",
    "\n",
    "- check that each each image is seen at each slot in the 192\n",
    "- write a test function to get the count of how many times an image is seen in each slot \n",
    "- ideal counterbalancing: each image is seen in each slot so that we eliminate trial order effects \n",
    "\n",
    "\n",
    "### Matched VE sequence\n",
    "\n",
    "- 16 imgs per block; shuffle order \n",
    "- 16 images should span the space of depth diff and depth bin \n",
    "- shuffle order of the blocks \n",
    "- duration rotation \n",
    "\n",
    "\n",
    "OR \n",
    "\n",
    "- Latin square: n x n \n",
    "    - row: participant\n",
    "    - col: trial number \n",
    "    - all 64 stimuli occur equally in each trial \n",
    "    - N = 64 participants \n",
    "    - Do this for discrimination and matched VE experiment sequences \n",
    "    - RANDLS function in MATLAB \n",
    "        - https://www.mathworks.com/matlabcentral/fileexchange/79200-random-latin-square\n",
    "        \n",
    "- Eliminate block structure \n",
    "\n",
    "\n",
    "12/20: Discrimination \n",
    "\n",
    "- 32 trials\n",
    "- latin square \n",
    "\n",
    "### To do\n",
    "\n",
    "- NEED TO RUN POWER ANALYSIS\n",
    "    - verbal judgement\n",
    "    - VE coded discrim \n",
    "- Calculate experiment time (include practice trials, exclude instructions) \n",
    "    - get sum of time so we know how long it takes \n",
    "    - right now set to 20 minutes \n",
    "    \n",
    "\n",
    "\n",
    "50, 100, 250, 1000 ms \n",
    "* test of shorter durations on VE and discrimination\n",
    "\n",
    "- look at Aude paper to see what is extracted at 70 ms\n",
    "- get a sense of the perceptual capability at the short durations \n",
    "\n",
    "\n",
    "50, 475, 1000 (3 durations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
