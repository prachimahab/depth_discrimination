{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrimination: Participant Exclusion and Replacement Pipeline\n",
    "\n",
    "1. The counterbalancing.csv contains every json that needs to be sampled for the discrimination and verbal judgement experiment (includes all durations) \n",
    "    - The row number for each sequence corresponds to the url fragment used in the variables file uploaded to Mechanical Turk \n",
    "    - This file does not change, the variables files is updated to resample sequences that get excluded\n",
    "2. First need to match up reported participant worker IDs to worker IDs reported in batch data \n",
    "    - All data files downloaded from the server need to be matched to a worker ID in batch data\n",
    "    - Data files that do not have a matched worker ID are moved to a seperate folder and are not analyzed \n",
    "3. Participant exclusion criteron are pre-registered on OSF (https://osf.io/28vjd) - if the participant is excluded, the counterbalanced sequence needs to be replaced in the variables file \n",
    "4. All participants who have completed need to be excluded from completing future HITs (exclude_workers.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Worker IDs from Batch data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/all_batch'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061\n"
     ]
    }
   ],
   "source": [
    "all_batch_worker_ids = []\n",
    "for path in os.listdir(batch_path):   \n",
    "    if 'csv' in path:\n",
    "        batch_data = pd.read_csv(batch_path + '/' + path)\n",
    "        batch_worker_ids = list(batch_data['WorkerId'])\n",
    "        all_batch_worker_ids += batch_worker_ids\n",
    "print(len(set(all_batch_worker_ids)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Worker IDs from Data Files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/data'\n",
    "\n",
    "workerid_filename_dict = {}\n",
    "worker_ids_from_data = []\n",
    "\n",
    "for file in os.listdir(datapath):\n",
    "    if 'csv' in file:\n",
    "        path = datapath + \"/\" + file\n",
    "        df = pd.read_csv(path, index_col=None, header=0)\n",
    "        worker_ids_from_data.append(df.workerId.unique()[0])\n",
    "        workerid_filename_dict[df.workerId.unique()[0]] = file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(worker_ids_from_data) == set(all_batch_worker_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# get the worker IDs that are in the data but NOT in the batch data\n",
    "# these data files should be moved to an archive and NOT analyzed\n",
    "batchdata_workerIDs = set(all_batch_worker_ids)\n",
    "move_files = []\n",
    "missing_wid = set([wid for wid in worker_ids_from_data if wid not in batchdata_workerIDs])\n",
    "\n",
    "print(len(missing_wid))\n",
    "for wid in missing_wid:\n",
    "    move_files.append(workerid_filename_dict[wid])\n",
    "print(len(move_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dir = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/data'\n",
    "# dest_dir = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/data_archive'\n",
    "\n",
    "# for file in move_files:\n",
    "#     shutil.move(current_dir + '/' + file, dest_dir + '/' + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCSVs(datafolder, num_discrim_trials):\n",
    "    \"\"\"\n",
    "    Combine all participant data into one pandas df\n",
    "    OR \n",
    "    Create df for single participant file \n",
    "    \n",
    "    returns:\n",
    "        (1) combined dataframe of all discrimination data \n",
    "        (2) combined dataframe of all scene property rating data \n",
    "    \"\"\"\n",
    "    #checks if path is a file\n",
    "    isFile = os.path.isfile(datafolder)\n",
    "\n",
    "    #checks if path is a directory\n",
    "    \n",
    "    isDirectory = os.path.isdir(datafolder)\n",
    "    \n",
    "    if isDirectory == True:\n",
    "        discrim_data = []\n",
    "        for filename in os.listdir(datafolder):\n",
    "            if 'csv' in filename:\n",
    "                path = datafolder + \"/\" + filename\n",
    "                df = pd.read_csv(path, index_col=None, header=0)\n",
    "                \n",
    "                df_discrim = df[0:num_discrim_trials]\n",
    "                discrim_data.append(df_discrim)\n",
    "\n",
    "        discrim_frame = pd.concat(discrim_data, axis=0, ignore_index=True)\n",
    "        \n",
    "    if isFile == True:\n",
    "        if 'csv' in datafolder:\n",
    "            df = pd.read_csv(datafolder, index_col=None, header=0)\n",
    "            df_discrim = df[0:num_discrim_trials]\n",
    "            discrim_data.append(df_discrim)\n",
    " \n",
    "    return discrim_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/data'\n",
    "num_total_trials = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_discrim = combineCSVs(data_path, num_total_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_discrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1054"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subjIDs = raw_discrim.subjID.unique()\n",
    "len(all_subjIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  24,   28,   31,   38,   45,   33,   35,   34,   55,   27,   32,\n",
       "         40,   51,   58,   29,   36,   46,   25,   42,   48,   30,   37,\n",
       "         41,   26,   53,   50,   64, 1997,   61,   57,   47,   22,   23,\n",
       "         70, 1970, 1994,   56,   59,   52,   44,   21,   43,   69, 1974,\n",
       "         62,   49, 1990,   65, 1989,   39, 1960, 1965,   60, 1976,   63,\n",
       "         19,   54, 1985,   67,   20, 1971, 1963,   66, 1980,   77,   76,\n",
       "         71, 1973,   68, 1966, 1959, 1964])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ages = raw_discrim.age.unique()\n",
    "all_ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Data Cleaning </font> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchTrial_cleaning(df, correct_requirement, catch_stimuli):\n",
    "    '''\n",
    "    Participants complete 8 catch trials total to ensure that they are doing the task.\n",
    "    If less than 6/8 catch trials are correct, the participant is excluded.  \n",
    "    '''\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    remove = []\n",
    "    subj_sequence = {}\n",
    "    df2_list = []\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "        count_correct = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning\n",
    "        # \n",
    "        subj_sequence[subj] = subj_df.sequenceName.unique()[0]\n",
    "        \n",
    "        # remove trials that are outside the outlier range\n",
    "        c = 0\n",
    "        c_missed = 0\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            stim1 = row['stimulus_0']\n",
    "            stim2 = row['stimulus_1']\n",
    "            # TEMP SOLUTION FOR RANDOM PHP DATA SAVING PROBLEM \n",
    "            if type(stim1) == str:\n",
    "                if stim1.split('/')[1] in catch_stimuli or stim2.split('/')[1] in catch_stimuli:\n",
    "                    ####### VERSION WHERE CATCH TRIALS ARE ATTENTION CHECK: IMAGE 1 IS THE SAME AS IMAGE 2\n",
    "                    c += 1\n",
    "                    if row[\"discrim_choice\"] == 3:\n",
    "#                         print(row[\"discrim_choice\"])\n",
    "                        count_correct += 1\n",
    "#                     else:\n",
    "#                         print(row[\"discrim_choice\"])\n",
    "                    # remove catch trial \n",
    "                    cleaned_subj_df.drop([idx], inplace=True)\n",
    "    #                 print(depth0, depth1, correct_choice, choice)\n",
    "#         print(c_missed, 'Number of catch trials where participants did not see the target')\n",
    "#         print(c)\n",
    "#         print(count_correct)\n",
    "        if count_correct < correct_requirement:\n",
    "#             print('Number correct:', count_correct)\n",
    "            remove.append(subj)\n",
    "\n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    print(\"Number of participants that did not pass the catch trial check:\", len(remove))\n",
    "\n",
    "\n",
    "#     for index, row in df2.iterrows():\n",
    "#         if row['subjID'] in remove:\n",
    "#             df2.drop(index, inplace=True)\n",
    "            \n",
    "    for subj in remove:\n",
    "        df2.drop(df2[df2['subjID'] == subj].index, inplace = True) \n",
    "    \n",
    "    return df2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/discrim_jsons'\n",
    "sequences_count_dict = {}\n",
    "for seq in os.listdir(sequences_path):\n",
    "    if 'json' in seq:\n",
    "        sequences_count_dict['discrim_jsons/'+seq] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_catch_stim = ['000375_2014-06-08_11-17-29_260595134347_rgbf000133-resize_2',\n",
    "                  '000569_2014-06-09_22-51-47_260595134347_rgbf000141-resize_3',\n",
    "                  '000787_2014-06-08_22-33-53_260595134347_rgbf000175-resize_1',\n",
    "                  '002072_2014-06-24_21-48-06_260595134347_rgbf000115-resize_0',\n",
    "                  '001170_2014-06-17_15-43-44_260595134347_rgbf000096-resize_6',\n",
    "                  '001222_2014-06-17_16-24-06_260595134347_rgbf000073-resize_0',\n",
    "                  '001498_2014-06-19_17-45-14_260595134347_rgbf000129-resize_4',\n",
    "                  '001540_2014-06-20_17-01-05_260595134347_rgbf000086-resize_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants that did not pass the catch trial check: 574\n"
     ]
    }
   ],
   "source": [
    "catch_cleaned_discrim = catchTrial_cleaning(raw_discrim, 6, all_catch_stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_cleaned_data = catch_cleaned_discrim.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  28,   31,   38,   45,   33,   35,   34,   27,   40,   51,   58,\n",
       "         29,   36,   42,   55,   37,   25,   26,   50,   53,   61,   22,\n",
       "         46,   24, 1970,   30,   41,   56,   52,   44,   32,   43,   69,\n",
       "         47,   48,   62,   49,   64, 1989,   39,   21,   60,   63,   65,\n",
       "         57,   20,   67,   59,   66, 1980,   77,   54,   76,   23,   68])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_ages = catch_cleaned_discrim.age.unique()\n",
    "cleaned_ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RT_cleaning(df, outlier_range, num_trials):\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    for subj in all_subjIDs:\n",
    "        count = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning\n",
    "        \n",
    "        # calculate subject's average trial RT\n",
    "        average_trial_RT = subj_df[\"trial_RT\"].mean()\n",
    "        std_trial_RT = subj_df[\"trial_RT\"].std()\n",
    "        \n",
    "        # remove trials that are outside the outlier range\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            RT = row['trial_RT']\n",
    "            if RT < outlier_range[0]:\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "            if RT > outlier_range[1]:\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "                \n",
    "        threshold = math.floor(num_trials * 0.1)\n",
    "        if count >= threshold:\n",
    "            remove.append(subj)\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    print(\"Number of Participants with 10% or more trials outside their RT range:\", len(remove))\n",
    "    \n",
    "#     for index, row in df2.iterrows():\n",
    "#         if row['subjID'] in remove:\n",
    "#             df2.drop(index, inplace=True)\n",
    "            \n",
    "    for subj in remove:\n",
    "        df2.drop(df2[df2['subjID'] == subj].index, inplace = True) \n",
    "    \n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with 10% or more trials outside their RT range: 15\n"
     ]
    }
   ],
   "source": [
    "cleaned_discrim = RT_cleaning(catch_cleaned_discrim, [250,10000], 78)\n",
    "\n",
    "step2_cleaned_data = cleaned_discrim.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalTrialCountCheck(df, num_trials):\n",
    "    \"\"\"\n",
    "    If more then 10% of a participants data is missing, remove the participant\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\\n\",\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    remove = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_trials = len(subj_df.index)\n",
    "        threshold_trials_remaining = num_trials - math.floor(num_trials * 0.1)\n",
    "        \n",
    "        if count_trials <= threshold_trials_remaining:\n",
    "            remove.append(subj)\n",
    "    print(\"Number of Participants with >= 10% trials removed:\", len(remove))\n",
    "    \n",
    "#     for index, row in df.iterrows():\n",
    "#         if row['subjID'] in remove:\n",
    "#             df.drop(index, inplace=True)\n",
    "            \n",
    "    for subj in remove:\n",
    "        df.drop(df[df['subjID'] == subj].index, inplace = True) \n",
    "    \n",
    "    print(\"Number of participants left:\",len(df.subjID.unique()))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with >= 10% trials removed: 0\n",
      "Number of participants left: 465\n"
     ]
    }
   ],
   "source": [
    "final_discrim = finalTrialCountCheck(cleaned_discrim, 78)\n",
    "final_data = final_discrim.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_participant_count = len(final_data.subjID.unique())\n",
    "final_participant_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Sequence Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the version for the sequence tracking \n",
    "\n",
    "prev_version = 'v9'\n",
    "new_version = 'v10'\n",
    "\n",
    "# select path for the last previous sequence tracking file \n",
    "\n",
    "sequence_sampling_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/sequence_tracking/'+ prev_version + '_Discrim_master_sequence_tracking.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_sequence_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences previously sampled:  464\n",
      "Number of sequences sampled now:  465 465\n",
      "Number of sequences to be sampled:  3\n",
      "Number sampled + to be sampled = 468:  True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Opening JSON file\n",
    "f = open(sequence_sampling_path)\n",
    "  \n",
    "# returns JSON object as a dictionary\n",
    "sequence_sampling = json.load(f)\n",
    "# print number of sequences that have been sampled by the previous batch\n",
    "prev_sampled_count = 0\n",
    "for seq in sequence_sampling:\n",
    "    if len(sequence_sampling[seq]) > 0:\n",
    "        prev_sampled_count += 1\n",
    "print('Number of sequences previously sampled: ', prev_sampled_count)       \n",
    "\n",
    "new_sequence_sampling = sequence_sampling\n",
    "# update sequence sampling dictionary\n",
    "for subj in final_data.subjID.unique():\n",
    "    subj_df = final_data.loc[final_data['subjID'] == subj]\n",
    "    subj_seq = subj_df.sequenceName.unique()[0].split('/')[1]\n",
    "    # add subj to list for its corresponding sequence\n",
    "    new_sequence_sampling[subj_seq].append(str(subj))\n",
    "\n",
    "    \n",
    "sampled_count = 0\n",
    "unsampled_count = 0\n",
    "for seq in new_sequence_sampling:\n",
    "    if len(new_sequence_sampling[seq]) > 0:\n",
    "        # remove duplicates of the same id\n",
    "        new_sequence_sampling[seq] = list(set(new_sequence_sampling[seq]))\n",
    "        sampled_count += 1\n",
    "    else:\n",
    "        unsampled_count += 1\n",
    "        new_sequence_sampling[seq] = []\n",
    "        \n",
    "print('Number of sequences sampled now: ', sampled_count, len(final_data.sequenceName.unique()))\n",
    "\n",
    "print('Number of sequences to be sampled: ', unsampled_count)\n",
    "\n",
    "print('Number sampled + to be sampled = 468: ', sampled_count + unsampled_count==468)\n",
    "\n",
    "\n",
    "seq_track_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/sequence_tracking/'\n",
    "\n",
    "with open(seq_track_path + new_version + \"_Discrim_master_sequence_tracking.json\", \"w\") as outfile:\n",
    "    json.dump(new_sequence_sampling, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_sequence_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find sequences to replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, True)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_to_replace = []\n",
    "\n",
    "for seq_key in new_sequence_sampling:\n",
    "    if len(new_sequence_sampling[seq_key]) == 0:\n",
    "        sequences_to_replace.append(seq_key)\n",
    "\n",
    "len(sequences_to_replace), unsampled_count == len(sequences_to_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new batch variables file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sampled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>discrim_jsons/Discrim250_randls_70_rotated.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>discrim_jsons/Discrim250_randls_25_rotated.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>discrim_jsons/Discrim125_randls_17.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>discrim_jsons/Discrim125_randls_5_rotated.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>discrim_jsons/Discrim1000_randls_20.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>discrim_jsons/Discrim250_randls_44_rotated.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>discrim_jsons/Discrim250_randls_71.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>discrim_jsons/Discrim1000_randls_58.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>discrim_jsons/Discrim250_randls_16_rotated.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>discrim_jsons/Discrim250_randls_43_rotated.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path  Sampled\n",
       "0    discrim_jsons/Discrim250_randls_70_rotated.json        0\n",
       "1    discrim_jsons/Discrim250_randls_25_rotated.json        0\n",
       "2            discrim_jsons/Discrim125_randls_17.json        0\n",
       "3     discrim_jsons/Discrim125_randls_5_rotated.json        0\n",
       "4           discrim_jsons/Discrim1000_randls_20.json        0\n",
       "..                                               ...      ...\n",
       "463  discrim_jsons/Discrim250_randls_44_rotated.json        0\n",
       "464          discrim_jsons/Discrim250_randls_71.json        0\n",
       "465         discrim_jsons/Discrim1000_randls_58.json        0\n",
       "466  discrim_jsons/Discrim250_randls_16_rotated.json        0\n",
       "467  discrim_jsons/Discrim250_randls_43_rotated.json        0\n",
       "\n",
       "[468 rows x 2 columns]"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterbalancing_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/counterbalancing.csv'\n",
    "counterbalancing_df = pd.read_csv(counterbalancing_path)\n",
    "counterbalancing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Discrim250_randls_0.json',\n",
       " 'Discrim125_randls_73_rotated.json',\n",
       " 'Discrim125_randls_19_rotated.json']"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross-check with server console log\n",
    "sequences_to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1000 = 0\n",
    "count_250 = 0\n",
    "count_125 = 0\n",
    "for seq in sequences_to_replace:\n",
    "    if 'Discrim1000' in seq:\n",
    "        count_1000 += 1\n",
    "    if 'Discrim250' in seq:\n",
    "        count_250 += 1\n",
    "    if 'Discrim125' in seq:\n",
    "        count_125 += 1\n",
    "\n",
    "count_1000, count_250, count_125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Notes\n",
    "\n",
    "The row in the counterbalancing csv does NOT match the url fragment since the indexing includes the path row.\n",
    "\n",
    "The url fragment is the counterbalancing df index + 1 --> this has been validated in the console log of the experiment\n",
    "\n",
    "To backtrack from the url fragments to the corresponding row of the counterbalancing csv: row = url_fragment + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fragments = []\n",
    "for sequence in sequences_to_replace:\n",
    "    seq_p = 'discrim_jsons/' + sequence\n",
    "    url_fragments.append(counterbalancing_df.index[counterbalancing_df['Path']==seq_p][0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number for the NEXT batch \n",
    "batch = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_variables_csv = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/mturk_batch_variables/'\n",
    "\n",
    "base_url = 'http://54.235.29.9/FacialAge/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/v2_DepthDiscrim_HTML.html#'\n",
    "\n",
    "variables = {'experiment_url': []}\n",
    "\n",
    "for fragment in url_fragments:\n",
    "     variables['experiment_url'].append(base_url + str(fragment))\n",
    "\n",
    "variables_df = pd.DataFrame(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_df.to_csv(dest_variables_csv + 'depth_discrimination_variables' + '_b' + str(batch) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
